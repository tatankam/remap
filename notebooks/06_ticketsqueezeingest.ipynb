{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf93470",
   "metadata": {},
   "source": [
    "### compare csv file before ingest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e026a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161502 159885\n",
      "Added 6650\n",
      "Removed 8267\n",
      "Common 153235\n",
      "old duplicates: 0\n",
      "new duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# run in the same environment you used before\n",
    "import pandas as pd\n",
    "df_old = pd.read_csv(\"23112025affiliate-98__TicketSqueeze-Events.csv\", dtype=str).fillna(\"\")\n",
    "df_new = pd.read_csv(\"30112025affiliate-98__TicketSqueeze-Events.csv\", dtype=str).fillna(\"\")\n",
    "key = \"event_id\"   # adjust if different\n",
    "\n",
    "len_old = len(df_old)\n",
    "len_new = len(df_new)\n",
    "\n",
    "old_keys = set(df_old[key])\n",
    "new_keys = set(df_new[key])\n",
    "\n",
    "added = new_keys - old_keys\n",
    "removed = old_keys - new_keys\n",
    "common = old_keys & new_keys\n",
    "\n",
    "print(len_old, len_new)\n",
    "print(\"Added\", len(added))\n",
    "print(\"Removed\", len(removed))\n",
    "print(\"Common\", len(common))\n",
    "\n",
    "# check uniqueness\n",
    "print(\"old duplicates:\", df_old.duplicated(subset=[key]).sum())\n",
    "print(\"new duplicates:\", df_new.duplicated(subset=[key]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b5c415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7391182', 'ticket_lowprice', '33.81', '29.03'),\n",
       " ('7123016', 'ticket_lowprice', '44.08', '36.63'),\n",
       " ('7349978', 'ticket_lowprice', '145.72', '65.10'),\n",
       " ('6796220', 'ticket_lowprice', '72.98', '73.76'),\n",
       " ('7367674', 'ticket_lowprice', '76.59', '35.65'),\n",
       " ('7543392', 'ticket_lowprice', '38.71', '20.68'),\n",
       " ('7577592', 'ticket_lowprice', '78.20', '45.92'),\n",
       " ('7116580', 'ticket_lowprice', '120.18', '114.70'),\n",
       " ('7460478', 'ticket_lowprice', '45.24', '42.08'),\n",
       " ('7238235', 'ticket_lowprice', '97.23', '92.86')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 10 example keys that were added / removed / changed\n",
    "list(added)[:10]\n",
    "list(removed)[:10]\n",
    "\n",
    "# sample a few changed keys (if you want to inspect)\n",
    "def row_by_key(df, k):\n",
    "    return df[df[key] == k].iloc[0].to_dict()\n",
    "\n",
    "sample_changed = []\n",
    "for k in list(common)[:100]:   # check first 100 common to find diffs\n",
    "    r_old = row_by_key(df_old, k)\n",
    "    r_new = row_by_key(df_new, k)\n",
    "    for col in df_old.columns:\n",
    "        if col == key:\n",
    "            continue\n",
    "        if r_old.get(col, \"\") != r_new.get(col, \"\"):\n",
    "            sample_changed.append((k, col, r_old.get(col), r_new.get(col)))\n",
    "            break\n",
    "sample_changed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8779ae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected key columns: ['event_id']\n",
      "Old rows: 161502, New rows: 159885\n",
      "Added: 6650, Removed: 8267, Common: 153235\n",
      "Old rows: 161502, New rows: 159885\n",
      "Added: 6650, Removed: 8267, Common: 153235\n",
      "\n",
      "✓ Wrote delta file: delta.csv\n",
      "  Added: 6650\n",
      "  Removed: 8267\n",
      "  Changed: 67071\n",
      "\n",
      "✓ Wrote delta file: delta.csv\n",
      "  Added: 6650\n",
      "  Removed: 8267\n",
      "  Changed: 67071\n"
     ]
    }
   ],
   "source": [
    "# Create an accurate delta.csv between two CSVs (normalizes values before comparing)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- Config ---\n",
    "old_file = \"23112025affiliate-98__TicketSqueeze-Events.csv\"\n",
    "new_file = \"30112025affiliate-98__TicketSqueeze-Events.csv\"\n",
    "out_file = \"delta.csv\"\n",
    "normalize_case = True      # lower-case text before comparing\n",
    "normalize_whitespace = True\n",
    "drop_key_duplicates = True # if key duplicates exist, keep first and warn\n",
    "# ----------------\n",
    "\n",
    "def detect_key(df):\n",
    "    for c in df.columns:\n",
    "        if c.lower() == \"event_id\":\n",
    "            return [c]\n",
    "    for c in df.columns:\n",
    "        if c.lower() == \"id\":\n",
    "            return [c]\n",
    "    return [df.columns[0]]\n",
    "\n",
    "def normalize_series(s: pd.Series):\n",
    "    # operate on strings: strip, collapse spaces, optionally lower\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    if normalize_whitespace:\n",
    "        s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    else:\n",
    "        s = s.str.strip()\n",
    "    if normalize_case:\n",
    "        s = s.str.lower()\n",
    "    return s\n",
    "\n",
    "def normalize_df(df: pd.DataFrame):\n",
    "    df2 = df.copy()\n",
    "    for c in df2.columns:\n",
    "        df2[c] = normalize_series(df2[c])\n",
    "    return df2\n",
    "\n",
    "p_old = Path(old_file)\n",
    "p_new = Path(new_file)\n",
    "if not p_old.exists() or not p_new.exists():\n",
    "    raise FileNotFoundError(\"One of the input files does not exist\")\n",
    "\n",
    "# Read as strings to avoid dtype surprises\n",
    "df_old = pd.read_csv(p_old, dtype=str).fillna(\"\")\n",
    "df_new = pd.read_csv(p_new, dtype=str).fillna(\"\")\n",
    "\n",
    "key_cols = detect_key(df_old)\n",
    "print(f\"Detected key columns: {key_cols}\")\n",
    "\n",
    "# Ensure key exists\n",
    "for k in key_cols:\n",
    "    if k not in df_old.columns or k not in df_new.columns:\n",
    "        raise ValueError(f\"Key column '{k}' not found in both files\")\n",
    "\n",
    "# Check duplicates and optionally drop\n",
    "dups_old = df_old.duplicated(subset=key_cols).sum()\n",
    "dups_new = df_new.duplicated(subset=key_cols).sum()\n",
    "if dups_old or dups_new:\n",
    "    print(f\"Warning: duplicates found — old:{dups_old} new:{dups_new}. Using first occurrence.\")\n",
    "    if drop_key_duplicates:\n",
    "        df_old = df_old.drop_duplicates(subset=key_cols, keep='first')\n",
    "        df_new = df_new.drop_duplicates(subset=key_cols, keep='first')\n",
    "\n",
    "# Keep original copies for output\n",
    "df_old_orig = df_old.copy()\n",
    "df_new_orig = df_new.copy()\n",
    "\n",
    "# Normalize copies for robust comparisons\n",
    "df_old_norm = normalize_df(df_old)\n",
    "df_new_norm = normalize_df(df_new)\n",
    "\n",
    "# Build lookup dicts: key-tuple -> row (Series)\n",
    "def key_of_row(row, keys):\n",
    "    return tuple(row[k] for k in keys)\n",
    "\n",
    "old_map_norm = { key_of_row(r, key_cols): r for _, r in df_old_norm.iterrows() }\n",
    "new_map_norm = { key_of_row(r, key_cols): r for _, r in df_new_norm.iterrows() }\n",
    "old_map_orig = { key_of_row(r, key_cols): r for _, r in df_old_orig.iterrows() }\n",
    "new_map_orig = { key_of_row(r, key_cols): r for _, r in df_new_orig.iterrows() }\n",
    "\n",
    "old_keys = set(old_map_norm.keys())\n",
    "new_keys = set(new_map_norm.keys())\n",
    "added_keys = sorted(new_keys - old_keys)\n",
    "removed_keys = sorted(old_keys - new_keys)\n",
    "common_keys = sorted(old_keys & new_keys)\n",
    "\n",
    "print(f\"Old rows: {len(df_old)}, New rows: {len(df_new)}\")\n",
    "print(f\"Added: {len(added_keys)}, Removed: {len(removed_keys)}, Common: {len(common_keys)}\")\n",
    "\n",
    "records = []\n",
    "# Added: include original new row prefixed 'new_'\n",
    "if added_keys:\n",
    "    rows = [new_map_orig[k] for k in added_keys]\n",
    "    df_added = pd.DataFrame(rows)\n",
    "    df_added = df_added.rename(columns={c: f\"new_{c}\" for c in df_added.columns if c not in key_cols})\n",
    "    df_added['delta_type'] = 'added'\n",
    "    records.append(df_added)\n",
    "\n",
    "# Removed: include original old row prefixed 'old_'\n",
    "if removed_keys:\n",
    "    rows = [old_map_orig[k] for k in removed_keys]\n",
    "    df_removed = pd.DataFrame(rows)\n",
    "    df_removed = df_removed.rename(columns={c: f\"old_{c}\" for c in df_removed.columns if c not in key_cols})\n",
    "    df_removed['delta_type'] = 'removed'\n",
    "    records.append(df_removed)\n",
    "\n",
    "# Changed: compare normalized values column-by-column and only mark if any non-key differs\n",
    "changed_rows = []\n",
    "for k in common_keys:\n",
    "    old_row_norm = old_map_norm[k]\n",
    "    new_row_norm = new_map_norm[k]\n",
    "    # check any non-key column differs in normalized form\n",
    "    differs = False\n",
    "    for col in df_old_norm.columns:\n",
    "        if col in key_cols:\n",
    "            continue\n",
    "        v_old = old_row_norm[col]\n",
    "        v_new = new_row_norm.get(col, \"\") if col in df_new_norm.columns else \"\"\n",
    "        if pd.isna(v_old) and pd.isna(v_new):\n",
    "            continue\n",
    "        if str(v_old) != str(v_new):\n",
    "            differs = True\n",
    "            break\n",
    "    if differs:\n",
    "        old_row_orig = old_map_orig[k]\n",
    "        new_row_orig = new_map_orig[k]\n",
    "        rec = {}\n",
    "        # key columns kept as-is (original values)\n",
    "        for kc_idx, kc in enumerate(key_cols):\n",
    "            rec[kc] = k[kc_idx]\n",
    "        # add old_ and new_ original columns for visibility\n",
    "        for col in df_old_orig.columns:\n",
    "            if col not in key_cols:\n",
    "                rec[f\"old_{col}\"] = old_row_orig[col]\n",
    "        for col in df_new_orig.columns:\n",
    "            if col not in key_cols:\n",
    "                rec[f\"new_{col}\"] = new_row_orig[col]\n",
    "        rec['delta_type'] = 'changed'\n",
    "        changed_rows.append(rec)\n",
    "\n",
    "if changed_rows:\n",
    "    df_changed = pd.DataFrame(changed_rows)\n",
    "    records.append(df_changed)\n",
    "\n",
    "if not records:\n",
    "    print('No differences found — creating empty delta file')\n",
    "    pd.DataFrame().to_csv(out_file)\n",
    "else:\n",
    "    df_out = pd.concat(records, ignore_index=True, sort=False).fillna(\"\")\n",
    "    # order columns: keys, delta_type, then the rest\n",
    "    rest = [c for c in df_out.columns if c not in key_cols + ['delta_type']]\n",
    "    cols_order = list(key_cols) + ['delta_type'] + rest\n",
    "    cols_order = [c for c in cols_order if c in df_out.columns]\n",
    "    df_out = df_out[cols_order]\n",
    "    df_out.to_csv(out_file, index=False)\n",
    "    print('\\n✓ Wrote delta file:', out_file)\n",
    "    print('  Added:', len(added_keys))\n",
    "    print('  Removed:', len(removed_keys))\n",
    "    print('  Changed:', len(changed_rows))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
